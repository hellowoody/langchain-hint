{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5849305-f5e9-473f-8573-fdde105fb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "from typing import List, Optional\n",
    "\n",
    "class ChatGLM2(LLM):\n",
    "    max_token: int = 2048\n",
    "    temperature: float = 0.1\n",
    "    top_p = 0.7\n",
    "    history = []\n",
    "    \n",
    "    def __init__(self, temperature=temperature):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ChatGLM2\"\n",
    "    \n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        data = json.dumps({\n",
    "            'prompt':prompt,\n",
    "            'temperature':self.temperature, \n",
    "            'history':self.history,\n",
    "            'max_length':self.max_token})\n",
    "        print(\"ChatGLM prompt:\", prompt)\n",
    "        # 调用api \n",
    "        response = requests.post(\"http://192.168.16.150:8000\", headers=headers, data=data, timeout=30)\n",
    "        if response.status_code!=200:\n",
    "            return \"查询结果错误\"\n",
    "        resp = response.json()\n",
    "        if stop is not None:\n",
    "            response = enforce_stop_tokens(response, stop)\n",
    "        self.history = self.history + [[None, resp['response']]]\n",
    "        return resp['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "153cd538-8649-41de-95b5-1dd59876258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = ChatGLM2(temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e671b63-1222-4ff3-a13a-fe1966c491b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLM prompt: \n",
      "    内容生成任务：请根据下面的提示，生成一段文本。\n",
      "    \n",
      "    小明老婆昨天生了一对龙凤胎，他们还有一个4岁的女儿，那么现在小明一共有几个孩子？分别是几个儿子，几个女儿？\n",
      "\n",
      "小明老婆昨天生了一对龙凤胎，他们还有一个4岁的女儿。所以，小明现在一共有三个孩子。其中，两个是儿子，一个女儿。\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "    内容生成任务：请根据下面的提示，生成一段文本。\n",
    "    \n",
    "    {text}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "   input_variables=[\"text\"],\n",
    "   template=template,\n",
    ")\n",
    "text = '小明老婆昨天生了一对龙凤胎，他们还有一个4岁的女儿，那么现在小明一共有几个孩子？分别是几个儿子，几个女儿？'\n",
    "prompt_format = prompt.format(text=text)\n",
    "output = llm.invoke(prompt_format)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb4fb9ab-2e73-41de-996b-5a3770e03980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLM prompt: AI: Hi.\n",
      "System: Your role is a program coder.\n",
      "Human: 小明老婆昨天生了一对龙凤胎，他们还有一个4岁的女儿，那么现在小明一共有几个孩子？分别是几个儿子，几个女儿？\n",
      "AI: 现在小明一共有三个孩子。分别是两个儿子和一个女儿。\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "messages = [\n",
    "    AIMessage(content=\"Hi.\"),\n",
    "    SystemMessage(content=\"Your role is a program coder.\"),\n",
    "    HumanMessage(content=\"小明老婆昨天生了一对龙凤胎，他们还有一个4岁的女儿，那么现在小明一共有几个孩子？分别是几个儿子，几个女儿？\"),\n",
    "]\n",
    "output = llm.invoke(messages)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd552d-97b7-4de2-86a3-55d0e46dea7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
