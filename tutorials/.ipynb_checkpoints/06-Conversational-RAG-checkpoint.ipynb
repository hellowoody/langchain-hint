{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0409be9-256b-4978-a479-a3bd8abb3ca7",
   "metadata": {},
   "source": [
    "# 对话式 RAG - Conversational RAG\n",
    "在许多问答应用程序中，我们希望允许用户进行来回对话，这意味着应用程序需要某种过去问题和答案的“记忆”，以及将这些问题和答案纳入当前思维的逻辑。\n",
    "\n",
    "在本指南中，我们重点介绍如何添加逻辑来整合历史消息。此处介绍了有关聊天记录管理的更多详细信息。\n",
    "\n",
    "我们将介绍两种方法：\n",
    "\n",
    " - 链 Chains，我们始终在其中执行检索步骤；\n",
    "\n",
    " - 代理 Agents，我们让 LLM 自行决定是否以及如何执行检索步骤（或多个步骤）。\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7d19642-69c3-4146-a4e7-040507b030a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5d171d3-a7fc-40f2-bf5b-4c40c29e33b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_community.embeddings import BaichuanTextEmbeddings\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     base_url=\"http://api.baichuan-ai.com/v1\",\n",
    "#     api_key=os.environ[\"BAICHUAN_API_KEY\"],\n",
    "#     model=\"Baichuan4\",\n",
    "# )\n",
    "\n",
    "# embeddings = BaichuanTextEmbeddings(baichuan_api_key=os.environ[\"BAICHUAN_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ec92521-9ea0-46a9-9be4-2e886b6a588d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ZHIPUAI_API_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m      4\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(\n\u001b[1;32m      5\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://open.bigmodel.cn/api/paas/v4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBAICHUAN_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      7\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglm-4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings(\n\u001b[1;32m     11\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://open.bigmodel.cn/api/paas/v4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m---> 12\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mZHIPUAI_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding-2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_learn/lib/python3.9/os.py:679\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    676\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ZHIPUAI_API_KEY'"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://open.bigmodel.cn/api/paas/v4\",\n",
    "    api_key=os.environ[\"ZHIPUAI_API_KEY\"],\n",
    "    model=\"glm-4\",\n",
    ")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    base_url=\"https://open.bigmodel.cn/api/paas/v4\",\n",
    "    api_key=os.environ[\"ZHIPUAI_API_KEY\"],\n",
    "    model=\"Embedding-2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "225bf96f-89cb-407b-bc0f-fc5830ab05e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://ouzhoubei.co/article-794-1.html\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(id=(\"article_content\"))\n",
    "    )\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits,embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "system_prompt = (\n",
    "    \"您是问答任务的助手。\"\n",
    "    \"使用以下检索到的上下文来回答问题。如果您不知道答案，请说您不知道。最多使用三句话并保持答案简洁。\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",system_prompt),\n",
    "    (\"human\",\"{input}\")\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm,prompt)\n",
    "rag_chain = create_retrieval_chain(retriever,question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7af69e7-1b8e-49b5-81cc-3d8bf9e8fb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'意大利队在2024年欧洲杯的具体球员名单没有在提供的上下文中提及，因此无法确定他们的确切球员数量。通常国家队会有一支23至26人的球员阵容参加国际大赛，但具体数字需要查看官方公布的名单。'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"意大利队有多少位球员?\"})\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb665b6-8b84-4ca5-9961-294c12997e9d",
   "metadata": {},
   "source": [
    "请注意，我们使用了内置链构造函数 create_stuff_documents_chain 和 create_retrieval_chain，因此我们的解决方案的基本要素是：\n",
    "\n",
    " - retriever\n",
    " - prompt\n",
    " - LLM\n",
    "   \n",
    "这将简化合并聊天历史记录的过程。\n",
    "\n",
    "## 添加聊天记录\n",
    "\n",
    "我们构建的链直接使用输入查询来检索相关上下文。但在对话设置中，用户查询可能需要对话上下文才能被理解。例如，考虑以下交流：\n",
    "\n",
    "> 人类：“什么是任务分解？”\n",
    ">\n",
    "> 人工智能：“任务分解涉及将复杂的任务分解为更小、更简单的步骤，以便代理或模型更易于管理。”\n",
    ">\n",
    "> 人类：“常见的分解方法有哪些？”\n",
    "\n",
    "为了回答第二个问题，我们的系统需要理解“它”指的是“任务分解”。  \n",
    "我们需要更新现有应用的两件事：  \n",
    "\n",
    " - Prompt 提示：更新我们的提示以支持历史消息作为输入。\n",
    "\n",
    " - Contextualizing questions 情境化问题：添加一个子链，该子链接受最新的用户问题并在聊天历史的上下文中重新表述它。这可以简单地被认为是构建一个新的“历史感知”检索器。  \n",
    "   而之前我们有：\n",
    "\n",
    "   -  query查询 -> retriever检索器\n",
    "     \n",
    "      现在我们将有：\n",
    "      \n",
    "   - （query查询，conversation history对话历史）-> LLM大模型 -> rephrased query重新表述的查询 -> retriever检索器\n",
    "  \n",
    "**将问题情境化 Contextualizing the question**  \n",
    "\n",
    "首先，我们需要定义一个子链，该子链接收历史消息和最新的用户问题，并且如果问题引用了历史信息中的任何信息，则重新表述该问题。  \n",
    "\n",
    "我们将使用一个提示模板，其中包含一个名为 \"chat_history\" 的 MessagesPlaceholder 变量。  \n",
    "这使我们能够使用 \"chat_history\" 输入键向提示传递消息列表，这些消息将在系统消息之后和包含最新问题的人类消息之前插入。 \n",
    "\n",
    "请注意，我们利用辅助函数 create_history_aware_retriever 来完成此步骤，该函数管理 chat_history 为空的情况，否则按顺序应用 prompt | llm | StrOutputParser() | 检索器。  \n",
    "\n",
    "create_history_aware_retriever 构造一个链，该链接受键 input 和 chat_history 作为输入，并具有与检索器相同的输出模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "962303e2-1e30-4d60-ac89-4fd89f5a6620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"根据聊天记录和用户最新的问题，\"\n",
    "    \"该问题可能涉及聊天历史中的上下文，\"\n",
    "    \"形成一个独立的问题，\"\n",
    "    \"这个问题是可以在不了解聊天历史的情况下被理解的。\"\n",
    "    \"不要回答这个问题，只需在需要时进行重新表述，否则保持原样返回。\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",contextualize_q_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\",\"{input}\")\n",
    "])\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(llm,retriever,contextualize_q_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef25ba9-acb5-4c54-9b75-2848e68dcc4d",
   "metadata": {},
   "source": [
    "此链将在我们的检索器前添加输入查询的改写，从而让检索过程融入对话的上下文信息。\n",
    "\n",
    "现在我们可以构建完整的QA问答链条了。这个过程很简单，只需将检索器更新为我们新创建的history_aware_retriever即可。\n",
    "\n",
    "同样，我们将使用create_stuff_documents_chain来生成一个question_answer_chain，  \n",
    "其输入键包括context（上下文）、chat_history（聊天历史）和input（输入）——它接收检索到的上下文，连同对话历史和查询一起，来生成答案。\n",
    "\n",
    "我们通过create_retrieval_chain来构建最终的RAG（检索增强生成）链条。这条链条依次应用history_aware_retriever和question_answer_chain，为了方便起见，会保留诸如检索到的上下文等中间输出。  \n",
    "该链条的输入键包括input（输入）和chat_history（聊天历史），其输出则包含input（输入）、chat_history（聊天历史）、context（上下文）和answer（答案）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "822ffef5-dce8-4906-8759-99494b2c65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\",\"{input}\")\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm,qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever,question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf3e71-2feb-473b-ad87-91f9bcd22f45",
   "metadata": {},
   "source": [
    "让我们尝试一下。下面我们将提出一个问题以及一个需要结合上下文才能给出合理回答的跟进问题。  \n",
    "由于我们的链条包含了一个\"chat_history\"（聊天历史）输入，调用者需要管理这个聊天历史。  \n",
    "我们可以通过将输入和输出的消息追加到一个列表中来实现这一点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b7c1b8b-5595-40d2-9b43-907f2f9ac6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "其中有两位来自尤文图斯俱乐部，分别是20号的扎卡尼和24号的坎比亚索。\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage,HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"意大利队有多少位球员?\"\n",
    "ai_msg_1 = rag_chain.invoke({\n",
    "    \"input\":question,\n",
    "    \"chat_history\":chat_history\n",
    "})\n",
    "\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question),\n",
    "    AIMessage(content=ai_msg_1[\"answer\"])\n",
    "])\n",
    "\n",
    "question2 = \"其中有几位来自尤文图斯俱乐部？\"\n",
    "ai_msg_2 = rag_chain.invoke({\n",
    "    \"input\":question2,\n",
    "    \"chat_history\":chat_history\n",
    "})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316c8bed-6f69-4e04-9f0b-266e29c85bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
